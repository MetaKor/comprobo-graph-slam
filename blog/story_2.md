# Project Story 2

## GraphSLAM in Python: data structures!

Now that we have acquired a pretty good grasp on how GraphSLAM works as explored in our [previous blog post](https://github.com/MetaKor/comprobo_graph_slam/blob/main/blog/story_1.md), it is time to dive into the question of how to represent the requisite mathematical objects in Python code. In many ways this seems to be one of the toughest tasks when implementing a mathematical algorithm, as there is so much flexibility in how one could choose to programmatically represent things. This is traditionally thought of as the realm of "data structures and algorithms," and one of our learning goals for this project was diving into this challenge since none of us have ever taken a formal DSA class and we would like more practice in this area, particularly when working with the geometric objects like pose transformations that come up frequently in robotics.

We entered this phase of the project with the optimistic intentions of coding our own complete ground-up implementation of GraphSLAM representation and optimization. We had followed along with [Grisetti et al’s 2016 tutorial](http://dx.doi.org/10.1109/MITS.2010.939925), which is written with the intention of offering a sufficiently detailed step-by-step explanation of the algorithm and mathematical objects such that one could implement it in code. We thus hoped we could implement it fairly directly. However, after spending several hours attempting to do this, we realized that the translation step that still needed to happen between the formal mathematical description in the tutorial realm and a concrete, well-designed Python implementation was significantly larger than anticipated. We decided that pushing forward with this translation effort would not be the best use of our time, because we knew that we would learn much less from hacking together a poorly-designed Python interface than we would from exploring how others with more software experience had accomplished the same task, and then figuring out how to interface with their implementation.

### Learning from the pros

Fortunately, a bit of searching yielded a gem of a package for us. Jeff Irion wrote this [python-graphslam package](https://github.com/JeffLIrion/python-graphslam) in 2020, directly based on the Grisetti tutorial. The package has quite well-designed interfaces and manages to implement the tutorial’s representations and algorithms in clean and robust idiomatic object-oriented Python while staying very readable and interpretable. Even better, it’s almost totally void of high-level usage documentation, which forces us to really read the code and class docstrings to understand how to create and manipulate instances of the package’s objects. Since this package basically represents the pinnacle of what we could have possibly hoped to accomplish in the timeframe of this project, it made the most sense to learn from and understand this package’s implementation and figure out how to use it in our Neato-ROS framework.

The python-graphslam package has 2 main classes, `Vertex` and `Edge`; instances of which compose the larger `Graph` object.
- A `Vertex` object is essentially a wrapper for a `Pose` objects, along with a few additional attributes such as a vertex ID/index and whether or not it is fixed (we fix the first vertex by convention as a consistent reference frame for our other vertices). More on poses right below!
- An `Edge` object connects two different vertices by storing their IDs along with a representation of the transformation connecting them; specifically, the estimate of this pose transformation and the information matrix denoting its uncertainty.

`Pose` objects really form the backbone of the entire package architecture. There is a `BasePose` superclass which forms the template for the four different pose classes we can use depending on our application. There are two 2-dimensional pose types, $\mathbb{R}^2$ for pure translations and $SE(2)$ for poses that also have orientation. Likewise, in 3D, we have $\mathbb{R}^3 and $SE(3)$ for poses without and with orientation data, respectively. Since we are dealing with a planar SLAM problem, we are only interested in the 2D representations.

![Lie group table](/blog/media/lie_groups.png)
*Aside: These 4 types of pose, along with others, are technically Lie groups as shown in the table above from [Sola et al's 2018 Micro Lie Theory](https://arxiv.org/abs/1812.01537). Lie theory gives us additional tools, especially in 3 dimensions, for using the properties of continuous manifolds to improve optimization.*

These `Pose` objects are powerful because they allow the optimization algorithm to be written for the most general case and thus seamlessly handle whatever type of pose we happen to be using in a specific implementation. This architecture has some of the following benefits:
- Pose composition, that is, the addition and subtraction of poses objects to represent transformations between them, can be expressed using the normal `+` and `-` operators in Python. While less useful in 2D, this is a powerful technique because it lets us represent the conceptual or abstract operation very naturally in code.
- Representation of the pose in various other forms is straightforward with the various methods belonging to the `Pose` classes. For example, this would make it easy to get quaternion or Euler angle representations of orientation for poses in $SE(3)$. Even in $SE(2)$, it makes it easy to get out a length-3 numpy array for the pose even though it is originally constructed with a 2-element list or array for translation and an additional orientation parameter.

Edges, meanwhile, had only 1 type implemented from the `BaseEdge` class, specifically `OdometryEdge`. It might have made sense to also have something like a `LandmarkEdge` (as [requested by someone else in a GitHub issue](https://github.com/JeffLIrion/python-graphslam/issues/23)!), but ultimately I think this was not implemented because only one type of edge is necessary so long as it can handle a pair of poses of the same arbitrary type. It then just becomes necessary to distinguish the actual odometry edges and landmark observation edges with different weights in the information matrix to denote the system’s confidence in each type of measurement.

### Information matrices give us additional power!

However, this architecture did present one slight caveat in our specific case. That is, the optimization step expects all vertices in the graph to be of the same type, since pose composition relies on the connected vertices to be the same. But in our case, odometry edges exist in $SE(2)$ while measurement edges technically only exist in $\mathbb{R}^2$: since we are measuring the relative location of spherical landmarks, we have no measurement of the landmark orientation relative to the robot.

Fortunately this is not too tricky to overcome. We simply consider the measurements to be embedded in the $SE(2)$ (that is, simply set the landmark orientations to 0) and then be careful to assign an appropriate information matrix to these edges. By setting the third diagonal entry of the information matrix to 0 for these edges, the error calculation will not include any contribution from a difference in orientation and thus landmark orientation is not considered as desired.

$$\mathbf{\Omega}_{obs} = \begin{bmatrix}k_{tran} & 0 & 0 \\ 0 & k_{tran} & 0 \\ 0 & 0 & 0\end{bmatrix} \ \ \Rightarrow \ \ \mathbf{e}^T \mathbf{\Omega}_{obs} \mathbf{e} = k_{tran}(\Delta x) + k_{tran}(\Delta y) + \mathbf{0}(\Delta \theta)$$

Exploring these sorts of details in `python-graphslam` library was thus an immensely educational experience for us as budding representers of robotics algorithm in code with well-designed data structures!

## Robust Object Matching

The main problem I have been grappling within this part of the project is how to efficiently recognize and then match the objects in an image frame with the known objects we want to track. For simplicity, I decided that only monochromatic pool balls would be used, but this still proved challenging to implement. This challenge can be broken into two main aspects: identifying objects of interest in a live video feed and consistently matching an object with its proper match. Identifying an object within an image frame is difficult because any logic must be light enough that it doesn't freeze or impede the video stream itself, so the processing ideally should be as close to realtime as possible. In addition, the logic itself must consistently be able to identify an object in an efficent manner. Currently, we are using the Hough transformation method that attempts to fit a continous curve the an image gradient. Hough transforms can be used for straight or curved lines, but we are using a specific version meant for identifying circles in an image frame. Currently, I am running into many headaches with the function not returning any circles even when the sensitivity is lowered, but I hope to address this through processing the image input. Once the circles are identified, the center color of each of them will be compared to the primary color of the landmark objects. This is where the second challenge comes into place as a tolerance has to be set that excludes mismatches but also includes the training landmarks. If time allows I would like to include code that dynamically adjusts the tolerance until there is no change in the number of pixels; thus, the full object would be identified. All in all, I hope to soon move on to finding the angle and distance to the objects after they are identified in relation to the camera. 
