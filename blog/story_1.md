# Project Story 1

## The graphSLAM optimization technique zoo

At its core, the term "graphSLAM" refers more to a general worldview for approaching the SLAM problem than it does to any specific implementation. Indeed, there is a substantial variety in particular methods used to solve the SLAM problem given a graph-based representation of the robot's collected data.

While reading Thrun et al's treatment of the subject in Chapter 11 of *Probabilistic Robotics*, we learned that they take a somewhat non-standard approach to computing the maximum-likelihood trajectory and map. Specifically, they reduce the problem from one containing both motion links (describing the relation between subseuqent robot poses as measured by odometry) and observation links (describing the relation between a landmark and the pose at time of observation) to one with *exclusively* motion links. This lets them break up the problem into 2 distinct steps: trajectory optimization followed by map optimization. In other words, you can optimize the robot path in isolation (without having to consider the map!) and *then* use this maximum-likelihood trajectory to compute the map.

The way this is done is really interesting. When the robot collects data, it might look like the upper diagram: two subsequent robot poses (*A* and *B*) make independent observations of a single landmark. The poses (*A* and *B*) are connected with a solid motion link, while the poses are each connected to an observed version of this landmark (*a* and *b*) with dashed observation links, shown in the frame of the pose performing the measurement; only the landmark relative position (and not orientation) gets measured. Intuitively, taking the motion link as ground truth, these observations will almost certainly disagree since measurement is an inherently noisy process. The problem of graphSLAM is precisely managing such conflicts! While this motion-link-ground-truth is a fairly natural view of what's happening, we can shift our perspective as in the lower diagram. If we take our measurement links as ground truth, our conflicting landmark observations have been replaced with a conflicting pose estimate; one as measured by odometry, and the other as extrapolated from subsequent observations of the single landmark. In this manner, we can replace each pair of observations of the same landmark with an additional motion link. This results in a graph representation comprised solely of motion links, which substantially reduces the dimensionality of the inference problem and lets us compute a most-likely trajectory before constructing the map.

![Eliminating landmarks](/blog/media/landmark_elimination.jpeg)

While this landmark-elimination approach has the aforementioned advantages that might make it appealing on older hardware, more modern implementations tend to skip this step (which introduces a fair bit of conceptual overhead and mathematical machinery) and instead optimize direction on the complete pose-landmark graph. One advantage of this approach is that it lets the implementer have more explicit and direct control over how strongly odometry measurements versus landmark observation measurements are weighted in the graph optimization. The most basic way to do this direct optimization is via the Gauss-Newton method. More sophisticated algorithms like Levenberg-Marquardt offer improvements in convergence time and robustness by combining Gauss-Newton with gradient descent techniques.

We are actively in the process of implementing our graph representation and optimization methods, and as we are doing so we are working on a very basic "simulator" which lets us add artificial data to the graph so we can verify the optimization. This exploration of some of the interesting conceptual and theoretical underpinnings of how graphs represent SLAM data has proven very useful for helping develop the intuitions which are paramount to successfully implementing and debugging our version of graphSLAM!


## The struggle of efficient object detection

Another key component of our project is the detection, tracking, and relative position calculations between our NEATO and landmarks. In our case we are planning on using monocolor balls which should be fairly straightforward to detect using a circular Hough transform and identify with basic color matching. To tackle this problem, I am currently taking the approach of developing two classes. The first class, named 'objectdetect', allows the user to quickly add a new object to use as a landmark to track. This class takes an image of the object and then calculates the primary color, corners, descriptors, and other helpful factors to be stored as attributes. With these attributes, the second class called 'livetrack' will load in any objects to track, constantly detect them in the live video feed, and then use the known diameter of the object and relative size in frame to calculate the angle and distance to the Neato. So far, the objectdetect class has been mostly complete but I anticipate a lot of trig headaches trying to triangulate the polar coordinates from the neato to each object.